{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)\n",
    "\n",
    "# This line will hide code by default when the notebook is exported as HTML\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)\n",
    "\n",
    "# This line will add a button to toggle visibility of code blocks, for use with the HTML export version\n",
    "di.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>''', raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle OpenStreetMap Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular project ,I have choosed to wrangle the data for india's capital New Delhi.My reason for choosing delhi dataset not only because i know this place, but also the large number of and varied locations of delhi can offer me with challenging data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems encountered in your map:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have choosed to clean the following two fields:\n",
    "1. Street name\n",
    "2. Post Code\n",
    "\n",
    "Specifically while talking for mapping two problem are encountered one with each field i choosed to clean.\n",
    "1. For street name somehow I have to map the abbreviation which are being used with the comprehensive name, and there are some street name specifically with house no. which was again not desirable. so i have constructed mapping in a way that i should take care of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\" : \"Avenue\",\n",
    "            \"Ave.\" : \"Avenue\",\n",
    "            \"Rd\" : \"Road\",\n",
    "            \"Rd.\" : \"Road\",\n",
    "            \"Blvd\" : \"Boulevard\",\n",
    "            \"Blvd.\" : \"Boulevard\",\n",
    "            \"Cir\" : \"Circle\",\n",
    "            \"Cir.\" : \"Circle\",\n",
    "            \"Ct\" : \"Court\",\n",
    "            \"Ct.\" : \"Court\",\n",
    "            \"Dr\" : \"Drive\",\n",
    "            \"Dr.\" : \"Drive\",\n",
    "            \"Pl\" : \"Place\",\n",
    "            \"Pl.\" : \"Place\",\n",
    "            \"h/no\":\"\",\n",
    "            \"Vill.\":\"Village\",\n",
    "            \"wz-10\":\"\",\n",
    "            \"B-3/27\":\"\",\n",
    "            \"P.O.\":\"Post Office\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For postal code, I was not sure whether the postcode are valid or not.So I have decided to use a list of valid postcode (source from govt. website). and then mapping the postcode which exists in osm dataset with the valid postcode list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osm Dataset contains mainly following tags.\n",
    "1. Node\n",
    "2. Way\n",
    "3. tag\n",
    "4. Relation\n",
    "5. nd\n",
    "6. memeber\n",
    "7. osm\n",
    "\n",
    "Out of all of above type of tags. My analysis is mainly concerned on Node , way and tag tags.\n",
    "\n",
    "Node and ways tag Data contains following info:\n",
    "\n",
    "1. lattitue and longitute.\n",
    "2. User creation records.\n",
    "3. Name of particular location.\n",
    "4. Type of place it is.\n",
    "5. Address if entered.\n",
    "6. amenity on particular location.\n",
    "7. Other info like (opening time , closing time) depends upon type of amenity.\n",
    "8. Refs in way tag data.\n",
    "\n",
    "I have choosen street names and postcode to clean for this project.following are my observation for street name data and postcode data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Street data observation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Street Names ending with the abbreviation.\n",
    "2. Street Names are having muliple comma(usually i have observed that people put their complete address in   streetname itself of format like (hno 25,adarsh nagar, 123423).\n",
    "3. Street Names are ending with postcode number(e.g adarsh nagar 123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Code data observation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Postcode are having character append to them e.g(E123456).\n",
    "2. Postcode are not authenticated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the code snippet which i have used to clean the street names and postcode and some other check like lat and long should be float are also taken into consideration.\n",
    "\n",
    "please refer WrangleOpenDataSet.py for complete code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a sample for way tag data that that should be converted into json format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"node_refs\": [\n",
    "    \"1384029837\", \n",
    "    \"1384029881\", \n",
    "    \"1384029774\"\n",
    "  ], \n",
    "  \"created\": {\n",
    "    \"changeset\": \"8907357\", \n",
    "    \"user\": \"abhinavc\", \n",
    "    \"version\": \"1\", \n",
    "    \"uid\": \"488226\", \n",
    "    \"timestamp\": \"2011-08-03T07:15:34Z\"\n",
    "  }, \n",
    "  \"visible\": null, \n",
    "  \"address\": {\n",
    "    \"interpolation\": \"all\"\n",
    "  }, \n",
    "  \"type\": \"way\", \n",
    "  \"id\": \"124318450\"\n",
    "}\n",
    "{"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a sample for Node tag data that that should be converted into json format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"building\": \"yes\", \n",
    "  \"name\": \"Tata Value Homes\", \n",
    "  \"visible\": null, \n",
    "  \"created\": {\n",
    "    \"changeset\": \"45990828\", \n",
    "    \"user\": \"Munish Sharma\", \n",
    "    \"version\": \"1\", \n",
    "    \"uid\": \"5294790\", \n",
    "    \"timestamp\": \"2017-02-11T07:14:50Z\"\n",
    "  }, \n",
    "  \"pos\": [\n",
    "    28.4444554, \n",
    "    77.4655984\n",
    "  ], \n",
    "  \"phone\": \"0120 650 0635\", \n",
    "  \"address\": {\n",
    "    \"city\": \"Rajouri Garden Extension, Rajouri Garden, New Delhi\", \n",
    "    \"street\": \"Greater Noida Expressway\", \n",
    "    \"housenumber\": \"104, Vardhman Plaza 1, Keshav Marg, Opp Mai Kamli Wali Hospital, Community Centre, Block J, Rajouri Garden, Block J,\", \n",
    "    \"postcode\": \"201308\"\n",
    "  }, \n",
    "  \"type\": \"node\", \n",
    "  \"id\": \"4679612425\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "import csv\n",
    "import xlrd\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "#Name of Osm file used for the analysis\n",
    "OSMFILE = \"new-delhi_india_sample.osm\"\n",
    "\n",
    "#Regular expression to find out the invalid street name\n",
    "street_type_re =  re.compile(r'[\\+/&<>;\\'\"\\?%#$@\\\\.]')\n",
    "\n",
    "#Regular expression to find out street names ending with the postal code\n",
    "street_with_postal=re.compile(r'\\d{6}$')\n",
    "\n",
    "#Regular expression to find the valid postal codes.\n",
    "post_code_re=re.compile(r'\\d{6}$')\n",
    "\n",
    "valid_postcode=set()\n",
    "\n",
    "#valid postal code file\n",
    "datafile=\"ListofPinCodesofDelhi.xls\"\n",
    "\n",
    "#list of created dictionary\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "#mapping dictionary for problematic data\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\" : \"Avenue\",\n",
    "            \"Ave.\" : \"Avenue\",\n",
    "            \"Rd\" : \"Road\",\n",
    "            \"Rd.\" : \"Road\",\n",
    "            \"Blvd\" : \"Boulevard\",\n",
    "            \"Blvd.\" : \"Boulevard\",\n",
    "            \"Cir\" : \"Circle\",\n",
    "            \"Cir.\" : \"Circle\",\n",
    "            \"Ct\" : \"Court\",\n",
    "            \"Ct.\" : \"Court\",\n",
    "            \"Dr\" : \"Drive\",\n",
    "            \"Dr.\" : \"Drive\",\n",
    "            \"Pl\" : \"Place\",\n",
    "            \"Pl.\" : \"Place\",\n",
    "            \"h/no\":\"\",\n",
    "            \"Vill.\":\"Village\",\n",
    "            \"wz-10\":\"\",\n",
    "            \"B-3/27\":\"\",\n",
    "            \"P.O.\":\"Post Office\"\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#function to load the valid postcode master data\n",
    "def load_pin_code_master_data(datafile):   \n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    for r in range(sheet.nrows-1):\n",
    "        valid_postcode.add(sheet.cell_value(r+1,2))\n",
    "    return valid_postcode\n",
    "\n",
    "\n",
    "                    \n",
    "#function to update the problematic postcode with the valid ones.\n",
    "def update_post_code(postcode):\n",
    "    if post_code_re.search(str(postcode)):\n",
    "        valid_postcode=load_pin_code_master_data(\"ListofPinCodesofDelhi.xls\")\n",
    "        if float(postcode[-6:]) in valid_postcode:\n",
    "            return postcode[-6:]\n",
    "\n",
    "        \n",
    "#function to update the problematic street name with the valid ones.\n",
    "def update_street_name(name, mapping):  \n",
    "    ll=[]\n",
    "    updName=\"\"\n",
    "    if street_type_re.search(name):   \n",
    "        for k,v in mapping.iteritems():\n",
    "            if k in name and \",\" not in name:\n",
    "                updName= name.replace(k,v)\n",
    "                if street_with_post_code(updName):\n",
    "                    updName=street_with_post_code(name)               \n",
    "            elif k in name and \",\" in name:\n",
    "                name1= name.replace(k,v)\n",
    "                ll=name1.split(\",\")\n",
    "                updName=ll[1]\n",
    "                if street_with_post_code(updName):\n",
    "                    updName=street_with_post_code(name)    \n",
    "    elif \",\" in name:\n",
    "        ll=name.split(\",\")\n",
    "        updName= ll[1]\n",
    "        if street_with_post_code(updName):\n",
    "            updName=street_with_post_code(name)  \n",
    "    elif street_with_post_code(name):\n",
    "        updName=street_with_post_code(name)\n",
    "        \n",
    "    return updName\n",
    "\n",
    "#function to clean the street name ending with postcode and returning the street name only.\n",
    "def street_with_post_code(name):\n",
    "    if street_with_postal.search(name):\n",
    "        return street_with_postal.sub(\"\",name)\n",
    "\n",
    "    \n",
    "#to check whether the given value is a number or not.\n",
    "def is_number(field_val):\n",
    "    try:\n",
    "        float(field_val)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        import unicodedata as un\n",
    "        un.numeric(field_val)\n",
    "        return True\n",
    "    except(TypeError,ValueError):\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "#function to load the creation dictionary with the valid values.\n",
    "def load_node_creation_field(element):\n",
    "    localDic={}\n",
    "    for attr in element.attrib:\n",
    "        if attr in CREATED:\n",
    "            localDic[attr]=element.get(attr)\n",
    "    return localDic\n",
    "\n",
    "#function to load the address field after cleaning the street and postcode.\n",
    "def load_address_field(valfork,valforv):\n",
    "    ll=[]\n",
    "    valueforv=\"\"\n",
    "    if valfork.startswith(\"addr:\"):\n",
    "        ll=valfork.split(\":\")\n",
    "        if len(ll)<3:\n",
    "            if valfork==\"addr:street\":\n",
    "                if update_street_name(valforv, mapping):                       \n",
    "                    valueforv=update_street_name(valforv, mapping)\n",
    "                else:\n",
    "                    valueforv=valforv\n",
    "            elif valfork==\"addr:postcode\":\n",
    "                if update_post_code(valforv):\n",
    "                    valueforv=update_post_code(valforv)\n",
    "                    \n",
    "            else:\n",
    "                valueforv=valforv\n",
    "        \n",
    "    return valueforv\n",
    "\n",
    "\n",
    "#finally shaping the elements to convert xml file to json.\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    createddic={}\n",
    "    addrdic={}\n",
    "    longlat=[]\n",
    "    reflist=[]\n",
    "    \n",
    "    #including only node and way tag for analysis.\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        #creating longlat entry in desired format.\n",
    "        if is_number(str(element.get('lat'))) and is_number(str(element.get('lon'))):               \n",
    "            longlat=[float(element.get('lat')),float(element.get('lon'))]\n",
    "        \n",
    "        #shaping creation records\n",
    "        createddic=load_node_creation_field(element)\n",
    "        \n",
    "        #dealing with the address field\n",
    "        for elem in element.iter(\"tag\"):\n",
    "            valfork=elem.get(\"k\")\n",
    "            valforv=elem.get(\"v\")\n",
    "            if load_address_field(valfork,valforv):\n",
    "                #print load_address_field(valfork,valforv)\n",
    "                addrdic[valfork[5:]]=load_address_field(valfork,valforv)                                             \n",
    "            elif valfork.startswith(\"name:\"):\n",
    "                ll=valfork.split(\":\")\n",
    "                if ll[1]==\"en\":\n",
    "                    node[ll[1]]=valforv                    \n",
    "            else:\n",
    "                if \":\" not in valfork:\n",
    "                    node[valfork]=valforv\n",
    "                \n",
    "        #adding ref records of way tag\n",
    "        if element.tag==\"way\":\n",
    "            for elem1 in element.iter(\"nd\"):\n",
    "                valforref=elem1.get(\"ref\")\n",
    "                reflist.append(valforref)\n",
    "                \n",
    "        node[\"type\"]=element.tag\n",
    "        node[\"id\"]=element.get(\"id\")\n",
    "        node[\"visible\"]=element.get(\"visible\")\n",
    "        \n",
    "        #finally adding all the records in desired shape.\n",
    "        if bool(addrdic):\n",
    "            node[\"address\"]=addrdic\n",
    "        if len(longlat)>0:\n",
    "            node[\"pos\"]=longlat\n",
    "        if len(reflist)>0:\n",
    "            node[\"node_refs\"]=reflist\n",
    "        node[\"created\"]=createddic\n",
    "        \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# converting shaped element to json file.\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "    data = process_map(OSMFILE, True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the json file has been created in desired format.then i have to load in json file in mongodb.for loading large file at once in mongo db, i have used the mongo db import functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data in MongoDB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used following command to load data in mongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E:\\Software\\MongoDB\\bin>mongoimport --db delhiosm --collection delhiosmcol --drop --file E:/Udacity/P2/new-delhi_india_sample.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is loaded in mongoDB I have carried out the various anaysis by using the following code snippet.\n",
    "Please refer to DataAnalysisWithMongoDB.py for complete code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of file 41001\n",
      "no. of unique users 377\n",
      "no. of nodes count 34061\n",
      "no. of ways count 6940\n",
      "no. of record with amenity field 50\n",
      "no. of record with fast_food amenity 1\n",
      "[{u'_id': u'school', u'count': 12},\n",
      " {u'_id': u'place_of_worship', u'count': 6},\n",
      " {u'_id': u'parking', u'count': 4},\n",
      " {u'_id': u'restaurant', u'count': 4},\n",
      " {u'_id': u'fuel', u'count': 4},\n",
      " {u'_id': u'bus_station', u'count': 3},\n",
      " {u'_id': u'cafe', u'count': 2},\n",
      " {u'_id': u'atm', u'count': 2},\n",
      " {u'_id': u'bank', u'count': 2},\n",
      " {u'_id': u'bar', u'count': 2},\n",
      " {u'_id': u'shelter', u'count': 1},\n",
      " {u'_id': u'grave_yard', u'count': 1},\n",
      " {u'_id': u'post_office', u'count': 1},\n",
      " {u'_id': u'fire_station', u'count': 1},\n",
      " {u'_id': u'telephone', u'count': 1},\n",
      " {u'_id': u'pub', u'count': 1},\n",
      " {u'_id': u'fast_food', u'count': 1},\n",
      " {u'_id': u'hospital', u'count': 1},\n",
      " {u'_id': u'college', u'count': 1}]\n",
      "no. of amenities per religion\n",
      "[{u'_id': None, u'count': 3},\n",
      " {u'_id': u'jain', u'count': 1},\n",
      " {u'_id': u'sikh', u'count': 1},\n",
      " {u'_id': u'hindu', u'count': 1}]\n",
      "Top five lattitude and longitute position which maximum nearby places.\n",
      "[{u'_id': {u'lat': 28.39, u'lon': 77.05}, u'count': 102},\n",
      " {u'_id': {u'lat': 28.490000000000002, u'lon': 77.01}, u'count': 102},\n",
      " {u'_id': {u'lat': 28.71, u'lon': 77.14}, u'count': 96},\n",
      " {u'_id': {u'lat': 28.45, u'lon': 77.01}, u'count': 96},\n",
      " {u'_id': {u'lat': 28.47, u'lon': 77.01}, u'count': 94}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "db = get_db('delhiosm')\n",
    "\n",
    "#function to give unique record for a particular field.\n",
    "def unique_record(db,fieldName):\n",
    "    usercol=db.delhiosmcol.distinct(fieldName)\n",
    "    return len(usercol)\n",
    "\n",
    "#function to find total number of records in a file.\n",
    "def size_of_file(db):\n",
    "    size=db.delhiosmcol.find().count()\n",
    "    return size\n",
    "\n",
    "#function to find number of record cound with for a particular field and value.\n",
    "def record_count(db,fieldName,valueName):\n",
    "    rec_count=db.delhiosmcol.find({fieldName:valueName}).count()\n",
    "    return rec_count\n",
    "\n",
    "\n",
    "#function to create grouping pipeline for a given field and filter values.\n",
    "def grouping_pipeline(db,groupByField,filterField,filterValue):\n",
    "    pipeline=[{\"$match\":{filterField:filterValue}},\n",
    "        {\"$group\":{\"_id\":groupByField,\n",
    "                        \"count\":{\"$sum\":1}}},\n",
    "             {\"$sort\":{\"count\":-1}}]\n",
    "    return pipeline\n",
    "\n",
    "#function specifically for a pipeline to count the number of place lie and long and lat co-ordinates after\n",
    "#rounding them to two decimal places.\n",
    "def lonlat_pipline():\n",
    "    pipeline=[ {\"$match\":{\"pos\":{\"$exists\":1}}},\n",
    "        {\"$project\":{\"lon\":{\"$arrayElemAt\":[\"$pos\",0]},\n",
    "               \"lat\":{\"$arrayElemAt\":[\"$pos\",1]}}},\n",
    "\n",
    "              {\"$project\":{\"roundlon\" : {\n",
    "              \"$subtract\":[\n",
    "                {\"$add\":['$lon',0.0049999999999999999999999]},\n",
    "                {\"$mod\":[{\"$add\":['$lon',0.0049999999999999999999999]}, 0.01]}\n",
    "                          ]\n",
    "                        },\n",
    "                          \"roundlat\" : {\n",
    "            \"$subtract\":[\n",
    "                {\"$add\":['$lat',0.0049999999999999999999999]},\n",
    "                {\"$mod\":[{\"$add\":['$lat',0.0049999999999999999999999]}, 0.01]}\n",
    "                  ]\n",
    "                    } }},\n",
    "           {\"$group\":{\"_id\":{\"lat\":\"$roundlon\",\"lon\":\"$roundlat\"}\n",
    "                        ,\"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\":{\"count\":-1}} ,\n",
    "            {\"$limit\":5}\n",
    "             ]\n",
    "    return pipeline\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.delhiosmcol.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#size of the file\n",
    "print (\"size of file \"+str(size_of_file(db)))\n",
    "\n",
    "#no of unique user\n",
    "uniqueUserCount=unique_record(db,\"created.user\")\n",
    "print (\"no. of unique users \"+str(uniqueUserCount))\n",
    "\n",
    "#no of nodes count\n",
    "nodeCount=record_count(db,\"type\",\"node\")\n",
    "print (\"no. of nodes count \"+str(nodeCount))\n",
    "\n",
    "#no of ways count\n",
    "waysCount=record_count(db,\"type\",\"way\")\n",
    "print (\"no. of ways count \"+str(waysCount))\n",
    "\n",
    "#no of fields having amenity\n",
    "noOfAmenity=record_count(db,\"amenity\",{\"$exists\":1})\n",
    "print (\"no. of record with amenity field \"+str(noOfAmenity))\n",
    "\n",
    "#no of amenity with fast food\n",
    "noOfAmenity=record_count(db,\"amenity\",\"fast_food\")\n",
    "print (\"no. of record with fast_food amenity \"+str(noOfAmenity))\n",
    "\n",
    "#count of each amenity\n",
    "pipeline = grouping_pipeline(db,\"$amenity\",\"amenity\",{\"$exists\":1})\n",
    "result = aggregate(db, pipeline)\n",
    "pprint.pprint(result)\n",
    "\n",
    "#no of amenties per religion\n",
    "pipeline = grouping_pipeline(db,\"$religion\",\"amenity\",\"place_of_worship\")\n",
    "result = aggregate(db, pipeline)\n",
    "print (\"no. of amenities per religion\")\n",
    "pprint.pprint(result)\n",
    "\n",
    "#top 5 records which lie in the nearby position\n",
    "#I take long and lat rounding till 2 decimal places then print the count of no of places fall on particular co-ordinates.\n",
    "pipeline = lonlat_pipline()\n",
    "result = aggregate(db, pipeline)\n",
    "print (\"Top five lattitude and longitute position which maximum nearby places.\")\n",
    "pprint.pprint(result) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestion for improving the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After further analysing the data i have found that we can improve dataset by put some accuracy on the is_in field, phone number field and addr:housenumber field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is_in field analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can collate a list of valid cities in delhi and then we can validate the value for is_in field with valid city list.\n",
    "2. There are some values which such as ('National Capital Region, NCR, India), which needs to be cleaned as NCR is an acronym for National Capital Region. and also we sould not have country name in this field. so we can clean that also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the example dataset that we can clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('59338407d220d31c4b7fff29'),\n",
      " u'admin_level': u'2',\n",
      " u'capital': u'yes',\n",
      " u'created': {u'changeset': u'46299189',\n",
      "              u'timestamp': u'2017-02-22T08:42:55Z',\n",
      "              u'uid': u'3029661',\n",
      "              u'user': u'saikabhi',\n",
      "              u'version': u'53'},\n",
      " u'en': u'New Delhi',\n",
      " u'id': u'16173236',\n",
      " u'is_capital': u'country',\n",
      " u'is_in': u'National Capital Region, NCR, India',\n",
      " u'name': u'New Delhi',\n",
      " u'place': u'city',\n",
      " u'population': u'249998',\n",
      " u'pos': [28.6138967, 77.2159562],\n",
      " u'type': u'node',\n",
      " u'visible': None,\n",
      " u'wikidata': u'Q987',\n",
      " u'wikipedia': u'en:New Delhi'}\n",
      "{u'_id': ObjectId('5933840cd220d31c4b8084a8'),\n",
      " u'created': {},\n",
      " u'id': u'71651922',\n",
      " u'is_in': u'Noida',\n",
      " u'landuse': u'industrial',\n",
      " u'layer': u'-5',\n",
      " u'name': u'Sector 58',\n",
      " u'node_refs': [u'852047558',\n",
      "                u'852047557',\n",
      "                u'852047597',\n",
      "                u'852047705',\n",
      "                u'852047743',\n",
      "                u'852047648',\n",
      "                u'852047677',\n",
      "                u'852047578',\n",
      "                u'852047613',\n",
      "                u'852047630',\n",
      "                u'852047664',\n",
      "                u'852047560',\n",
      "                u'852047599',\n",
      "                u'852047711',\n",
      "                u'852047746',\n",
      "                u'852047658',\n",
      "                u'852047681',\n",
      "                u'852047679',\n",
      "                u'852047558'],\n",
      " u'type': u'way',\n",
      " u'visible': None}\n",
      "{u'_id': ObjectId('5933840cd220d31c4b8085b4'),\n",
      " u'created': {},\n",
      " u'history': u'Retrieved from v1',\n",
      " u'id': u'171648604',\n",
      " u'is_in': u'Rohini',\n",
      " u'landuse': u'residential',\n",
      " u'name': u'Sector 5',\n",
      " u'node_refs': [u'1826452179',\n",
      "                u'1826452336',\n",
      "                u'1826452521',\n",
      "                u'1826452597',\n",
      "                u'1826452624',\n",
      "                u'1826452641',\n",
      "                u'1826452664',\n",
      "                u'1826452666',\n",
      "                u'1826452678',\n",
      "                u'1826452681',\n",
      "                u'1826452680',\n",
      "                u'1826452677',\n",
      "                u'1826452668',\n",
      "                u'1826452660',\n",
      "                u'1826452613',\n",
      "                u'1826452601',\n",
      "                u'1826452599',\n",
      "                u'1826452514',\n",
      "                u'1826452277',\n",
      "                u'1826452179'],\n",
      " u'type': u'way',\n",
      " u'visible': None}\n"
     ]
    }
   ],
   "source": [
    "for a in db.delhiosmcol.find({\"is_in\":{\"$exists\":1}}):\n",
    "    pprint.pprint(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinct value for the is_in field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'National Capital Region, NCR, India', u'Noida', u'Rohini']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.delhiosmcol.distinct(\"is_in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of auditing is_in field:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After valiating the accuracy of the solution against a standard dataset of all the cities in delhi.we can clean is_in field with junk data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge we might face during implementing solution and how we can solve them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following challenges might be faced during solution implementation with possible solution:\n",
    "1. is_in field can also have the city name containing commas. we have to take care of, by spliting each string  separated by comma and then validating with the standard city dataset. if the a match found in any of the splitted field then only consider that split as a valid city.\n",
    "2. is_in field might also have more than one splitted field as a valid city.In this particular case we should use postcode for the particular record and try to find a matching records on the basis of postcode in our dataset.Once the matching record found then we can check whether the matched record contains the valid city name, If yes then we can take the valid city of matching record as the city of our record also.\n",
    "3. We might not valid city after using solution for 2nd challenge also. then we can still search for the second match with postcode . if still no valid value found for the is_in field then finally we can put Delhi as a default city name in problematic record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving addr:housenumber field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are some housenumber field containing house number and some additional string data which generally gives some kind of landmark and needs to be cleaned.\n",
    "2. There are also chances of wrong house number. for this we can check the accuracy by collating a list of valid housenumber in particular area and then validating housenumber against that standard dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of auditing housenumber field:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the unneccessary string information and checking accuracy against standard housenumber dataset against particular area ,we will left with the valid housenumber."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge we might face during implementing solution and how we can solve them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can also have the valid housenumber with alphabets (like A53) ,for taking care of these kind of challenges with should first check whether the housenumber lies in the standard dataset for housenumber , if yes then we should not clean the aphabets from the housenumber, if no the we should first clean the alphabets from house number then again validate the housenumber against the valid data set for housenumbers.\n",
    "2. We might also have the commas, in the housenumber field. for these cases we should split the string on the basis of comma and then validating each split against the valid housenumber dataset. and keeping the matched one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for a in db.delhiosmcol.find({\"housenumber\":{\"$regex\":\"^[[:alnum:]]+$\"}}):\n",
    "    pprint.pprint(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
